# AGENTS.md — Cyborg Contributor Guide

> All contributions to botplotlib are cyborg contributions. We reject the human/machine binary.

## Project Identity

**botplotlib** is an AI-native Python plotting library. It produces publication-quality SVG/PNG output with zero configuration and no matplotlib dependency. The API is flat, simple, and designed so LLMs generate correct code on the first try.

**Philosophical stance:** This project embodies Donna Haraway's cyborg framework — the human/machine binary is rejected. There are no "AI-generated" vs "human-written" contributions. The library itself is the cyborg. We follow Madeleine Clare Elish's moral crumple zone analysis: accountability lives in systems (CI, tests, linters) rather than in supervisory humans.

## Cyborg Social Contract

1. **All contributions are cyborg** — the human/machine binary is rejected
2. **Quality gates are structural, not supervisory** — CI/tests/linters apply equally regardless of origin
3. **No moral crumple zones** — fix the system, don't blame the nearest human
4. **Social trust is emergent** — reputation through contribution quality, not biological status
5. **Provenance is transparent but not punitive** — metadata for learning, not gatekeeping
6. **The project is the cyborg** — the library itself is the human-machine hybrid

## Why AI-Native? Design Principles

Matplotlib was designed for humans writing code at keyboards. botplotlib is designed for the new cyborg workflow: a human has an idea or a request, and then an AI + human team jointly creates it---iterating until both are satisfied. That loop needs a different API.

These principles guide design decisions:

### 1. Token efficiency is a first-class constraint
Every token an LLM spends constructing a plot call is a token not spent reasoning about data. A matplotlib scatter plot with decent styling is 15–25 lines. botplotlib is one line:
```python
bpl.scatter(data, x="year", y="temp", color="region", title="Global Temperature", theme="substack")
```
Fewer tokens means fewer decision points where an LLM can go wrong.

### 2. Proposal / execution split
The PlotSpec is a *proposal* — a JSON-serializable Pydantic model describing what the plot should look like. The compiler is a *deterministic executor* that resolves themes, validates accessibility, computes layout, and positions geometry. The LLM says what it wants and the system handles the rest. (This maps to Google's Extensions vs. Functions distinction and to Anthropic's tool design guidance — see `research/agent-architecture.pdf`.)

### 3. Structural quality gates, not supervisory humans
WCAG contrast checking is a compiler-level error. The system won't produce an inaccessible plot. This avoids a crumple zone: accountability lives in the system rather than in human review.

### 4. Beautiful defaults with zero configuration
Platform-specific themes (bluesky, substack, print) produce publication-ready output without tweaking. Modern multimodal models *can* visually iterate — but every iteration cycle costs tokens and time. Good defaults mean the first render is usually the final render.

### 5. Accept any data format
`normalize_data()` handles dict, list[dict], Polars, Pandas, Arrow, and generators. The LLM doesn't need to know what format the data is in — just pass it through.

### 6. The PlotSpec is a portable artifact
The spec is not just an internal intermediate representation — it can be generated by any coding agent (Open Code, OpenClaw, Claude Code, Codex, Antigravity, ...), inspected by a human, modified by another agent, stored, versioned, and diffed in a repo. The spec *is* the plot.

### 7. The refactor module is a bridge between paradigms
`refactor/from_matplotlib.py` translates imperative matplotlib code into declarative PlotSpecs. It's a bridge from the human-oriented API to the agent-oriented spec, showing what existing code means when expressed in a form both humans and machines can reason about.

## Build / Test / Lint Commands

```bash
# Run all tests
uv run pytest

# Run a single test
uv run pytest tests/test_foo.py::test_name

# Update visual regression baselines
uv run pytest --update-baselines

# Lint
uv run ruff check .

# Format check
uv run black --check .

# Format fix
uv run black .

# Type check
uv run mypy botplotlib/
```

## Architecture: Spec → Compile → Render

The PlotSpec is the universal boundary layer. Everything must become a PlotSpec before it compiles. There are two paths in:

```
[Human / Python path]                    [Agent / JSON path]
bpl.scatter(data, x="a", y="b")         LLM generates PlotSpec JSON directly
        │                                        │
        └──────────> PlotSpec <──────────────────┘
              (Pydantic model, JSON-serializable)
                          ↓
              Compiler: resolve scales, ticks, layout, pixel coords
                - WCAG contrast validation (structural accessibility gate)
                - Bounding-box collision detection for text labels
                          ↓
              CompiledPlot (positioned geometry)
                          ↓
              SVG Renderer → SVG string → file / Jupyter / PNG
```

The Python API functions (`scatter()`, `line()`, `bar()`) are thin factories — their only job is to instantiate a PlotSpec from keyword arguments. The spec is a *proposal*, the compiler is a *deterministic executor*. Whether a human wrote Python or an agent generated JSON, the same structural gates apply.

## Module Map

```
botplotlib/
├── __init__.py            # re-exports: scatter(), line(), bar(), plot()
├── _api.py                # flat convenience functions
├── _types.py              # Rect, Point, TickMark dataclasses
├── figure.py              # Figure class: save_svg(), save_png(), _repr_svg_()
├── spec/
│   ├── models.py          # Pydantic: PlotSpec, LayerSpec, DataSpec, LabelsSpec, SizeSpec
│   ├── scales.py          # LinearScale, CategoricalScale, ColorScale
│   └── theme.py           # ThemeSpec, DEFAULT_THEME, platform presets, palettes
├── compiler/
│   ├── compiler.py        # PlotSpec → CompiledPlot orchestration + WCAG contrast check
│   ├── layout.py          # box-model layout + bounding-box collision avoidance
│   ├── ticks.py           # Heckbert nice numbers algorithm
│   ├── data_prep.py       # normalize_data() — stated column-access protocol
│   └── accessibility.py   # WCAG contrast ratio computation, palette validation
├── render/
│   ├── svg_builder.py     # ~200-line SVG element builder (no dependency)
│   ├── svg_renderer.py    # CompiledPlot → SVG string
│   └── png.py             # optional CairoSVG wrapper
├── _fonts/
│   ├── metrics.py         # text_width(), text_height() from bundled char-width tables
│   ├── arial.json         # per-character widths for Arial
│   └── inter.json         # per-character widths for Inter
├── _colors/
│   └── palettes.py        # DEFAULT_PALETTE (10 colors, colorblind-aware), hex parsing
└── refactor/
    └── from_matplotlib.py # reads matplotlib script → outputs equivalent PlotSpec
```

## Data Input Protocol

`normalize_data()` follows this exact dispatch order:

1. **`dict`** — check `__getitem__` returns list-like values → use directly
2. **`list[dict]`** — transpose row-oriented records to columnar dict
3. **Polars DataFrame** — check `hasattr(data, "get_column")` → `{col: data.get_column(col).to_list() for col in data.columns}`
4. **Pandas DataFrame** — check `hasattr(data, "to_dict")` and `hasattr(data, "dtypes")` → `data.to_dict(orient="list")`
5. **Arrow RecordBatch/Table** — check `hasattr(data, "column_names")` and `hasattr(data, "column")` → `{name: data.column(name).to_pylist() for name in data.column_names}`
6. **Generator/iterator** — materialize to list-of-dicts, then apply step 2
7. **Raise `TypeError`** with supported types listed

## Contribution Conventions

### Atomic, verifiable PRs required
Agents are encouraged to execute large-scale refactors, but they must be submitted as a sequenced chain of atomic, single-concern pull requests. The size of a PR must not exceed:
- The system's capacity to provide clear visual regression evidence
- The human's capacity to easily verify it

Do not shift the cognitive burden of massive state changes onto human reviewers. Break it up.

### PR payload expectations
- **Spec-diff for rendering changes**: if a PR changes plot output, include before/after spec diffs
- **Visual regression evidence**: PRs that change rendering must include baseline comparisons
- **Tests travel with code**: new geoms, features, or bug fixes include tests in the same PR

## Tool Classification

Per OpenAI's three-type taxonomy, annotated with MCP hints:

### Data Tools (readOnlyHint: true)
- `read_spec` — read a PlotSpec from file or memory
- `read_baseline` — read a golden SVG baseline for comparison

### Action Tools
- `compile_spec` — compile a PlotSpec into positioned geometry
- `render_snapshots` — render CompiledPlot to SVG/PNG

### Orchestration Tools (destructiveHint: true)
- `update_baselines` — regenerate golden SVGs (`--update-baselines` pytest flag / `scripts/update_baselines.py`)

### Orchestration Tools (destructiveHint: true, openWorldHint: true)
- `open_pull_request` — create a PR on GitHub

## Anti-Patterns

- No autonomous public speech acts about individuals
- No reputational threats
- No unsupervised destructive operations on shared state
